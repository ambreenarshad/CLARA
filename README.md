# NLP Agentic AI Feedback Analysis System

A production-grade multi-agent AI system for feedback analysis using LangChain, ChromaDB, and FastAPI with sentiment analysis, topic modeling, and RAG capabilities.

## ğŸš€ Project Status

**Current Phase:** âœ… **PRODUCTION READY** (All 3 Iterations Complete)
**Version:** 1.0.0
**Status:** Fully Operational

## âœ¨ Features

- ğŸ¤– **Multi-Agent Architecture** - 4 specialized LangChain agents working in harmony
- ğŸ“Š **Sentiment Analysis** - VADER-powered emotion detection
- ğŸ” **Topic Modeling** - BERTopic automatic theme discovery
- ğŸ“ **Text Summarization** - TextRank extractive summaries
- ğŸ” **RAG Retrieval** - Semantic search with ChromaDB
- ğŸš€ **FastAPI Backend** - Async, high-performance REST API
- ğŸ§ª **Comprehensive Testing** - Unit + integration tests
- ğŸ³ **Docker Ready** - Complete containerization
- ğŸ“š **API Documentation** - Interactive Swagger UI

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Agent Framework** | LangChain |
| **API** | FastAPI (Async) |
| **Vector Store** | ChromaDB |
| **Embeddings** | sentence-transformers (all-MiniLM-L6-v2) |
| **Sentiment** | VADER |
| **Topic Modeling** | BERTopic + UMAP |
| **Summarization** | TextRank (spaCy) |
| **Testing** | pytest + pytest-asyncio |
| **Containerization** | Docker + Docker Compose |

## ğŸ“‹ Quick Start

### Prerequisites

- Python 3.11+
- 4GB+ RAM
- Internet connection (first run only, for model downloads)

### Local Development

1. **Clone and setup**
```powershell
git clone <repo-url>
cd Project

# Create virtual environment
python -m venv .venv
.venv\Scripts\Activate.ps1  # Windows PowerShell
# source .venv/bin/activate  # Unix/Mac
```

2. **Install dependencies**
```powershell
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

3. **Configure environment**
```powershell
Copy-Item .env.example .env  # Windows
# cp .env.example .env  # Unix/Mac
```

4. **Run the application**
```powershell
python -m uvicorn src.api.main:app --reload
```

5. **Access the system**
- **API Base**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

### Docker Deployment

```bash
# Build and run
docker-compose up --build

# Verify health
curl http://localhost:8000/health
```

## ğŸ“¡ API Endpoints

### Core Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Welcome message |
| `/health` | GET | System health check |
| `/info` | GET | System information |

### Feedback Analysis

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/v1/upload` | POST | Upload feedback data |
| `/api/v1/analyze` | POST | Analyze existing feedback |
| `/api/v1/process` | POST | Upload + Analyze (one-step) |
| `/api/v1/feedback/{id}` | GET | Get feedback summary |
| `/api/v1/statistics` | GET | System statistics |

## ğŸ’¡ Usage Examples

### Quick Test with Sample Data

```bash
# Process sample feedback (60 entries)
curl -X POST http://localhost:8000/api/v1/process \
  -H "Content-Type: application/json" \
  -d @test_data/sample_feedback.json
```

### Python Example

```python
import requests

# Upload and analyze feedback
feedback_data = {
    "feedback": [
        "Excellent product! Highly recommend.",
        "Poor quality. Very disappointed.",
        "Good value for money."
    ]
}

response = requests.post(
    "http://localhost:8000/api/v1/process",
    json=feedback_data
)

result = response.json()
print(f"Sentiment: {result['sentiment']}")
print(f"Insights: {result['report']['key_insights']}")
```

**See [docs/API_USAGE_EXAMPLES.md](docs/API_USAGE_EXAMPLES.md) for complete API documentation.**

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Application             â”‚
â”‚         (src/api/main.py)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Agent Orchestrator              â”‚
â”‚    (Multi-Agent Coordinator)            â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚      â”‚      â”‚          â”‚
   â–¼      â–¼      â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Data â”‚ â”‚Ana â”‚ â”‚Ret  â”‚  â”‚Synth  â”‚
â”‚Ing. â”‚ â”‚lysisâ”‚ â”‚rievalâ”‚  â”‚esis â”‚
â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜
   â”‚      â”‚       â”‚         â”‚
   â–¼      â–¼       â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Core NLP Services               â”‚
â”‚  â€¢ VADER (Sentiment)                    â”‚
â”‚  â€¢ BERTopic (Topics)                    â”‚
â”‚  â€¢ TextRank (Summary)                   â”‚
â”‚  â€¢ sentence-transformers (Embeddings)   â”‚
â”‚  â€¢ ChromaDB (Vector Store)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

```
project_root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # 4 LangChain agents
â”‚   â”‚   â”œâ”€â”€ data_ingestion_agent.py
â”‚   â”‚   â”œâ”€â”€ analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ retrieval_agent.py
â”‚   â”‚   â”œâ”€â”€ synthesis_agent.py
â”‚   â”‚   â””â”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ api/                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ services/            # Core services
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”‚   â””â”€â”€ nlp_processors.py
â”‚   â””â”€â”€ utils/               # Configuration & utilities
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ exceptions.py
â”‚       â””â”€â”€ logging_config.py
â”œâ”€â”€ tests/                   # Complete test suite
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_nlp_processors.py
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ test_data/               # Sample data
â”‚   â””â”€â”€ sample_feedback.json
â”œâ”€â”€ docs/                    # Documentation
â”‚   â””â”€â”€ API_USAGE_EXAMPLES.md
â”œâ”€â”€ config.yaml              # Configuration
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ Dockerfile               # Docker config
â”œâ”€â”€ docker-compose.yml       # Docker Compose
â””â”€â”€ README.md                # This file
```

## ğŸ§ª Testing

### Run Tests

```bash
# All tests
pytest

# With coverage
pytest --cov=src --cov-report=html

# Specific test file
pytest tests/test_api.py

# Verbose output
pytest -v
```

### Test Coverage

- **Unit Tests**: NLP processors, agents, services
- **Integration Tests**: API endpoints, multi-agent workflows
- **Target Coverage**: 80%+

## âš™ï¸ Configuration

### config.yaml

Main configuration file:
```yaml
models:
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  spacy_model: "en_core_web_sm"

chromadb:
  persist_directory: "./chroma_db"
  collection_name: "feedback_embeddings"

api:
  host: "0.0.0.0"
  port: 8000

nlp:
  min_topic_size: 5
  max_topics: 10
  sentiment_threshold: 0.05
```

### Environment Variables

Create `.env` from `.env.example`:
```bash
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
CHROMA_PERSIST_DIR=./chroma_db
```

## ğŸ“Š What It Does

### 1. Data Ingestion
- Validates feedback text quality
- Cleans and normalizes text
- Stores in ChromaDB with embeddings
- Generates unique batch IDs

### 2. Sentiment Analysis
- VADER compound scoring
- Positive/Negative/Neutral classification
- Aggregated statistics
- Distribution analysis

### 3. Topic Modeling
- Automatic theme discovery
- Keyword extraction per topic
- Representative document identification
- Topic assignment for each feedback

### 4. Text Summarization
- Extractive summarization
- Key phrase extraction
- Configurable summary length

### 5. RAG Retrieval
- Semantic similarity search
- Context-aware retrieval
- Topic-based document matching

### 6. Report Generation
- Comprehensive insights
- Actionable recommendations
- Executive summaries
- Key findings highlights

## ğŸš€ Performance

- **Processing Speed**: ~100 feedback entries in 3-5 seconds
- **Memory Usage**: ~1-2GB for moderate datasets
- **Scalability**: Async processing for large batches
- **Storage**: Efficient vector embeddings (384 dimensions)

## ğŸ“š Documentation

- **[API Usage Guide](docs/API_USAGE_EXAMPLES.md)** - Complete API examples
- **[CLAUDE.md](CLAUDE.md)** - Architecture & implementation details
- **[Swagger UI](http://localhost:8000/docs)** - Interactive API docs

## ğŸ” Example Output

### Input
```json
{
  "feedback": [
    "Great product! Very satisfied.",
    "Terrible service. Will not recommend.",
    "Good value for money."
  ]
}
```

### Output
```json
{
  "sentiment": {
    "average_compound": 0.15,
    "sentiment_distribution": {
      "positive": 1,
      "neutral": 1,
      "negative": 1
    }
  },
  "report": {
    "key_insights": [
      "Mixed feedback: 33.3% positive, 33.3% negative",
      "Overall sentiment is neutral"
    ],
    "recommendations": [
      "Focus on addressing negative feedback themes"
    ]
  }
}
```

## ğŸ¯ Use Cases

- **Product Feedback Analysis** - Analyze customer reviews
- **Support Ticket Analysis** - Identify common issues
- **Survey Response Analysis** - Extract key themes
- **Social Media Monitoring** - Sentiment tracking
- **Employee Feedback** - HR insights
- **Market Research** - Competitor analysis

## ğŸ› Troubleshooting

### Common Issues

**1. Import errors**
```bash
# Solution: Ensure all dependencies installed
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

**2. Port already in use**
```bash
# Solution: Use different port
uvicorn src.api.main:app --port 8001
```

**3. ChromaDB errors**
```bash
# Solution: Clear database
rm -rf chroma_db/
```

## ğŸ‘¥ Team

6-person development team
CS4063 - Natural Language Processing
Development Track Project

## ğŸ“ License

Educational Project - CS4063 NLP Course

## ğŸ™ Acknowledgments

- **Course**: CS4063 Natural Language Processing
- **Technologies**: LangChain, FastAPI, ChromaDB, VADER, BERTopic
- **Models**: Hugging Face, spaCy

---

## ğŸ“ˆ Development Progress

```
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% Complete

âœ… Iteration 1: Foundation
âœ… Iteration 2: Agents & Pipeline
âœ… Iteration 3: Testing & Production
```

**Status**: Production Ready | **Version**: 1.0.0 | **Last Updated**: 2025-12-03

---

**Ready to analyze feedback!** ğŸš€

For questions or issues, check the [API documentation](docs/API_USAGE_EXAMPLES.md) or review the [complete architecture guide](CLAUDE.md).
